---
title: "Maximizing Efficiency in Under- and Recent Graduate Job Searching"
author: "ES3"
date: "December 9, 2017"
output: html_document
---

```{r, include=FALSE}
# install dependencies
#install.packages("dplyr")
#install.packages("ggplot2")
#install.packages("rpart")
#install.packages("rattle")
#install.packages("glmnet")
#install.packages("tidyr")

# import libraries
library(dplyr)
library(ggplot2)
library(rpart)
library(rattle)
library(glmnet)
library(tidyr)

# set working directory
#setwd("~/University of Washington/Senior/Fall/Info 370/project-es3")

# load data
data <- read.csv("data/clean_num.csv")
pilot <- read.csv("data/stress_pilot.csv")

```

# Decision Context
As college students nearing the end of our undergraduate careers, all four members of this group understand the pressure that comes with finding entry-level employment. And while a wide variety of resources are available to students looking for jobs, a pilot study we conducted on 33 self-selected participants from the University of Washington Information School showed that 78.8% still found the job search process to be stressful to some degree.

```{r, echo=FALSE}
# rename column headers
colnames(pilot) <- c("class", "stress")

# order class sequentially
pilot$class <- factor(pilot$class, levels=c("Alumni", "5th year", "Senior", "Junior", "Sophomore"))

# plot stress data
ggplot(data = pilot) +
  geom_bar(mapping = aes(x = stress), width = 0.5) +
  labs(title = "Job Search Stress") +
  coord_flip(ylim = c(0, nrow(pilot)))

```

Our goal is therefore to illuminate the effectiveness of different job search resources so that students can allocate their time most productively. Our model focuses on making recommendations that minimize excess effort and time time spent job searching. These include recommendations about what skills you should develop in the long term and what resources you should use in the short term.

# Model
Our original model approached our question as a classification questions. Respondents were segmented into two groups: those who accepted a job offer in under 3 months of searching, and those who did not. We chose the 3 month cutoff because about half of respondents fell on one side and half on the other.

```{r}
og.model.data <- data %>%
  # reformat graduation data as date
  mutate(graduation_date = as.Date(graduation_date, "%m/%d/%Y")) %>%
  
  # split job type into 3 binary columns (1 = internship, 2 = part time, 3 = full time)
  mutate(fulltime = ifelse(Job_type == 3, 1, 0)) %>%
  mutate(parttime = ifelse(Job_type == 2, 1, 0)) %>%
  mutate(internship = ifelse(Job_type == 1, 1, 0)) %>%
  select(-Job_type) %>%
  
  # convert months to classification variable (0 = >3mo., 1 = <3mo.)
  mutate(Months = ifelse(Months <= 3, 1, 0)) %>%
  
  # split class standing into 5 binary columns
  mutate(freshman = ifelse(class_standing_ == 1, 1, 0)) %>%
  mutate(sophomore = ifelse(class_standing_ == 2, 1, 0)) %>%
  mutate(junior = ifelse(class_standing_ == 3, 1, 0)) %>%
  mutate(senior = ifelse(class_standing_ == 4, 1, 0)) %>%
  mutate(fifth.year = ifelse(class_standing_ == 5, 1, 0)) %>%
  mutate(alumni = ifelse(class_standing_ == 6, 1, 0)) %>%
  select(-class_standing_) %>%
  
  # remove test data
  # (filter out people just beginning job search (no job, searching for < 3 mo.))
  filter((has_position == 0 & Months == 0) | (None_of_these == 1 & Months == 0) | has_position == 1 | None_of_these == 0) %>%
  
  # remove factors that we don't want to predict on
  select(-None_of_these, -has_position)

# create decision tree
og.tree <- rpart(Months ~ .,
                 data = og.model.data, method = "class",
                 control=rpart.control(minbucket=1))

# plot tree
fancyRpartPlot(og.tree)

```

Unfortunately, this model didn't particularly make very much sense. For instance, it suggests if you apply to fewer online job postings but use LinkedIn more, you are more likely to accept a job offer in under 3 months. But logically we expect both factors to be a part of the same strategy - perhaps online-based one - and therefore to be directly as opposed to inversely correlated. In the process of meeting with our mentor, it was brought to our attention that the model might be making confusing suggestions becuase it was built on data from individuals who could have pursued multiple strategies in their job search.

Our solution was to analyze only individuals who got offers in the first couple months of their job search under the assumption they would have only had the chance to utilize one strategy. We looked for a natural cutoff in the data but found none so four months was chosen arbitrarily. While this method prevents us from analyzing strategies that did not work it allows us to definitively point to the strategies that did work.

```{r}
revised.model.data <- data %>%
  # reformat graduation data as date
  mutate(graduation_date = as.Date(graduation_date, "%m/%d/%Y")) %>%
  
  # split job type into 3 binary columns (1 = internship, 2 = part time, 3 = full time)
  mutate(fulltime = ifelse(Job_type == 3, 1, 0)) %>%
  mutate(parttime = ifelse(Job_type == 2, 1, 0)) %>%
  mutate(internship = ifelse(Job_type == 1, 1, 0)) %>%
  select(-Job_type) %>%
  
  # split class standing into 5 binary columns
  mutate(freshman = ifelse(class_standing_ == 1, 1, 0)) %>%
  mutate(sophomore = ifelse(class_standing_ == 2, 1, 0)) %>%
  mutate(junior = ifelse(class_standing_ == 3, 1, 0)) %>%
  mutate(senior = ifelse(class_standing_ == 4, 1, 0)) %>%
  mutate(fifth.year = ifelse(class_standing_ == 5, 1, 0)) %>%
  mutate(alumni = ifelse(class_standing_ == 6, 1, 0)) %>%
  select(-class_standing_) %>%
  
  # get subset where job found in <= 4 months
  filter(Months <= 4) %>%
  
  # remove test data
  # (filter out people just beginning job search (no job, searching for < 3 mo.))
  filter((has_position == 0 & Months == 0) | (None_of_these == 1 & Months == 0) | has_position == 1 | None_of_these == 0) %>%
  
  # remove factors that we don't want to predict on
  select(-None_of_these, -has_position)

# create decision tree
revised.tree <- rpart(Months ~ ., data = revised.model.data, method = "anova", control=rpart.control(minbucket=2))

# plot decision tree
fancyRpartPlot(revised.tree)

```

While not every aspect of the revised model makes perfect sense, the model's issues - unlike those in the original model - have more to do with our dataset's small sample size than with the model's design itself. See the limitations section for more on this.

# Additional Exploration
We did a lot of additional exploration on our dataset to supplement the model we created.

## Scatter Plot
First, we plotted scatter plot of a number of the months for job searching by a number of students at each type of job to determine when to stop job searching using the same method.
```{r}
ggplot(data = data) +
  geom_count(mapping = aes(x = Job_type, y = Months), alpha = 1/3) + scale_size_area(max_size = 10) + 
  labs(title = "Plot of Months by Number of Students in each Type of Job",
       x = "Types of Job", y = "Months Job Searching Took", size = "Number of People")

```
The scatter plot is suggesting that spending maximum of 7 months is adequate for job searching. Although some students got an offer spending more than a year, it is highly recommended to change the job searching method if the months spent on job search passes 7 months.

## Linear Regression Model
In order to analyze the job search resources that offer highest rates of a job, we implemented linear regression model. Iterating through the variables, we ran simple linear regression model to discover linear relationships between each resource as an input variable and the months of job searching as an output variable.
```{r}
#linear regression for each factor against the outcome variable (months it takes to find job)
#internship = 1, part time = 2, full time = 3
#female = 1, male = 0
for (i in 1:ncol(data)){
  print(ggplot(data, aes(x = data[[i]], y = Months)) +
    geom_point(alpha = 0.1) +
    geom_smooth(method=lm) +
      labs(x = colnames(data)[i]))
}

```
According to the simple linear regression models of resources, too much time input on one resource extends the time of getting a job. However, utilizing employer campus visit, employer website resource, Huskyjobs, and connection decreases the time of receiving an offer.

## Additional Linear Regression
There was unusual trends in our linear regression that points doubt on our expectation: Better major GPA results in more time of getting a job while cumulative GPA has opposite trends. Of course, it is crucial to maintain high cumulative GPA for employment. However, it is not logical to argue that the higher major GPA postpones the duration of job searching. Therefore, we plotted additional GPA graphs with removed values of 0.0.
```{R}
#additional gpa graphs to remove values of 0.0 for GPA
ggplot(data, aes(x = in.major_GPA, y = Months)) +
  geom_point(alpha = 0.1 ) + 
  xlim(2,4) +
  geom_smooth(method=lm) 

ggplot(data, aes(x = cumulative_GPA, y = Months)) +
  geom_point(alpha = 0.1 ) + 
  xlim(2,4) +
  geom_smooth(method=lm)

```
Unfortunately, there was no change in our results. We maintain that these are our limitations/variables resulted from data collection.

## Lasso Regression
For the multi-feature problem in linear regression, we used L1 regulation Lasso to model mean squared error.
```{r}
month <- data[, "Months"]

# Create dataframe 
df = data.frame(data$Months, data$resume_hrs, data$cover_letter_hours, data$self.confidence, data$online_job_postings, data$no_career_fairs, data$cumulative_GPA, data$in.major_GPA, data$internships)

x <- as.matrix(df[-1]) # Removes month
y <- as.matrix(df[, 1]) # Only month

# Plot Mean Squared Error Plot and Lasso path using glmnet
cvfit <- cv.glmnet(x, y,parallel=TRUE, standardize=TRUE, alpha=1, nlambda=1000)
fit <- glmnet(x, y, standardize=TRUE, alpha=1, nlambda=1000)
plot(fit)
plot(cvfit)

```
As lambda increases, the lasso removes more and more coefficients by setting them to zero. Also, there are two minima for graph: around log(lambda) of -1.3 with the number of non-zero coefficient 6 and the log(lambda) of 0 with the number of non-zero coefficient 1. Thus, the model is telling less number of factors makes better result for success in employment.

## Coefficient of Lasso Regression
We analyzed the coefficient of lasso model in order to observe the pattern.
```{r}
coef(fit)
```
The coefficient of lasso model shows increasing pattern of a number of career fairs student attended, despite decreasing pattern of cumulative GPA and number of internships. The number of career fairs, cumulative gpa, and internship is major factors that affect the result of employment.

## Scatter Plot
We plotted the observation from the coefficeint of lasso model 
```{r}
ggplot(data, aes(x= data$no_career_fairs, y = data$Months)) + geom_point(alpha = .1)
ggplot(data, aes(x= data$cumulative_GPA, y = data$Months)) + geom_point(alpha = .1)
ggplot(data, aes(x= data$internships, y = data$Months)) + geom_point(alpha = .1)
```
The graph tells us that more attendance at career fairs results in more number of months to get employed. However, there is an irony behind this explanation: It is a natural phenomenon that time spent on job application increases when a student goes to career fair often. Despite the irony, maintaining high cumulative GPA and internship experience is fatal that cumulative GPA and internships are highly related to the highest rates of job offers.

# Limitations

# Results
From our analysis we're able to make a few recommendations:
* Avoid overusing/relying too heavily on online resources like LinkedIn, online job postings, etc.
* If you are heavily dependent on online resources for one reason or another, solicit your friends/family/professors/etc. and see if anyone has connections to organizations of interest. Leverage the ones you find.
